<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<TITLE>Preliminary Program</TITLE>

<style>

BODY {
   MARGIN-TOP: 15pt;
   MARGIN-LEFT: 15pt;
   MARGIN-RIGHT: 15pt;
   MARGIN-BOTTOM: 15pt;
   FONT-SIZE: 10pt;
   FONT-FAMILY: "Times New Roman";
   BACKGROUND-COLOR: #ffffff;
   COLOR: #000000;
}

P {
   FONT-SIZE: 10pt;
}

TD {
   FONT-SIZE: 10pt;
}

TH {
   FONT-SIZE: 10pt;
}


A {
   COLOR: #32426c;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}
A:visited {
   COLOR: #32426c;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}
A:active {
   COLOR: #32426c;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
   TEXT-DECORATION: none
}
A:hover {
   COLOR: #32426c;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
   TEXT-DECORATION: underline
}
H4 {
   FONT-SIZE: 10pt;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}

H3 {
   FONT-SIZE: 11pt;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}

H2 {
   FONT-SIZE: 12pt;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}

H1 {
   FONT-SIZE: 14pt;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}

</style>

<p>

<h4>Automatic detection of surgical instruments´state in laparoscopic video images using neural networks.</h4>

<em>Celia Martín<sup>1</sup>,&nbsp;Ignacio Oropesa<sup>2</sup>,&nbsp;Juan Alberto Sánchez Margallo<sup>3</sup>,&nbsp;Francisco Miguel Sánchez Margallo<sup>3</sup>,&nbsp;Enrique J Gomez<sup>4</sup>,&nbsp;Patricia Sánchez<sup>5</sup></em><br>
<sup>1</sup>Universidad Politécnica de Madrid, <sup>2</sup>Grupo de Bioingeniería y Telemedicina. Universidad Politécnica de Madrid, <sup>3</sup>Centro de Cirugía de Mínima Invasión, <sup>4</sup>Universidad Politecnica de Madrid, <sup>5</sup>Grupo de Bioingenieria y Telemedicina. Universidad Politécnica de Madrid

<p>

<hr>


<h4>Abstract</h4>

<blockquote>
    <p>Software-based solutions such as virtual reality simulators and serious games
can be useful assets for training minimally invasive surgery technical skills.
However, their high cost and lack of realism/fidelity can sometimes be a
drawback for their incorporation in training facilities. In this sense, the
hardware interface plays an important role as the physical connection between
the learner and the virtual world. The EVA Tracking System, provides computer
vision-based information about the position and the orientation of the
instruments in an expensive and unobtrusive manner, but lacks information about
the aperture state of the clamps, which limits the system´s functionalities.
 This article presents a new solution for instrument´s aperture state
detection using artificial vision and machine learning techniques. To achieve
this goal, videos in a laparoscopic training box are recorded to obtain a data
set. In each frame, the instrument clamp is segmented in a region of interest
by means of color markers. The classifier is modeled using an Artificial Neural
Network. The trained prediction model obtains accuracy results of 94% in the
validation dataset and an error of 6% in independent evaluation video
sequences. Results show that the model provides a competent solution to
clamp´s aperture state detection. Future works will address the integration of
the model into the EVA and a virtual environment, the KTS serious game.</p> </p>
</blockquote>




<hr>

<p>
</body>
</html>